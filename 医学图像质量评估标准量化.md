# 医学图像质量评估标准量化

## 三、 Hallucinated-IQA-No-reference image quality assessment via adversarial learning.(CVPR2018)

**幻觉引导的图像质量评估：通过对抗性学习的无参照物图像质量评估**

> 参考网站：https://blog.csdn.net/qq_30159015/article/details/84580853
>
> https://blog.csdn.net/qq_34116958/article/details/98939214

### 摘要

> ==提出了一个伪参考图引导的质量回归网络来模拟人类视觉系统的行为，它可以利用失真的图像和伪参考图之间的感知差异信息来进行精确的预测。==首先在失真的图像上生成一个幻觉参考，以弥补真实参考的缺失。然后，将幻觉参照物的信息与扭曲的图像配对，并将它们转发给回归器，在生成器内隐含的排序关系的指导下学习感知差异，从而产生精确的质量预测。

作者==提出了通过GAN生成的伪参考图引导质量回归网络来解决无参考图像质量评价问题==，先在失真图像的基础上利用生成网络生成一个伪参考图，并通过判别器引导好的真实的伪参考图的生成，然后将失真图像、伪参考图像和失真图像的差输入到回归网络中，同时结合生成网络中的语义融合，最终预测产生图像的质量分数。==(对抗生成网络)==

### 网络架构

生成的伪参考图对最终的预测至关重要，糟糕的伪参考图会引入很大的偏差，从而导致质量回归结果变成次优值，因此通过两个机制来解决：

> 将==对抗性的思想==引入到伪参考图的生成和质量预测中，采用判别器；

> 为了进一步减少幻觉模型造成的质量回归网络的不稳定性，引入==高层语义融合机制==。它在生成网络中探索==隐含的排名关系==，作为一种指导，帮助回归网络以自适应的方式调整图像质量预测。

> (G不仅是一个发生器，也是一个编码器-解码器的机制。因此，以不同程度扭曲的图像之间的==差异信息==被紧凑地编码在编码器部分的末尾。我们把这种 "差异信息称为失真图像的"隐性排序关系")

![网络结构图](C:\Users\Administrator\Desktop\IMG_1775(20220228-124505).JPG)

#### 1. 生成网络G

输入为失真图像，生成一张伪参考图(Hallucinated image)。

> 生成网络G的总损失如下，其中$$L_P$$ 是像素损失，$$L_S$$是特征空间质量感知loss，$$L_{adv}$$是对抗损失:
>
> ![image-20220228130152729](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220228130152729.png)
>
> 其中对抗损失为：
>
> ![image-20220228130423095](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220228130423095.png)

> 前半部分$$l_p$$是伪参考图像与真实失真图像的==像素loss==，可以测量图像间的整体内容；后半部分$$l_s$$是==语义loss==，是伪参考图像和真实失真图像的感知差异，采用特征空间的损失作为感知约束：
>
> ![image-20220228130231683](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220228130231683.png)
>
> ![image-20220228130238468](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220228130238468.png)

> 上面的语义loss可以表示为如下公式，由语义差异和质量差异两部分组成：
>
> ![image-20220228125602346](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220228125602346.png)
>
> ![image-20220228125607235](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220228125607235.png)
>
> 其中$$l_v$$是由ImageNet预训练的VGG19网络第j层的特征loss，$$l_q$$是预测IQA得分的回归网络R在第k层的特征loss（这个做法源自于图像风格迁移），这两个loss就从特征层面上将VGG的语义信息和IQA的信息融入，保证生成器G的生成结果对于IQA更有效。



#### 2. 判别网络D

判别网络以==对抗==的方式与生成网络一起训练，==引导好的真实的伪参考图像的生成，抑制差的伪参考图像的生成==，以帮助生成网络产生更多合格的伪参考图。

传统的GAN网络是训练生成器来欺骗辨别器，训练辨别器来区别生成图像和真实图像。而在此算法中，最终目的是为了提高质量回归网络的性能，即使G不能生成高分辨率的幻觉图像，预测分数也应该是一个合理值，故而此时辨别器的作用是：==如果生成图像提高了回归网络的性能，那么将其视为真实图像，如果生成图像降低了回归网络的性能则将其视为假图像==。

==一个问题：网络图中判别器是将伪参考图和ground truth进行比较，但是为什么无参考的会有GT图？==

此模块中损失函数如下：

![image-20220228130544813](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220228130544813.png)

理解一下该公式，当回归网络预测出来的分数与真实质量分数的差距大于阈值时，伪参考图像降低了回归网络的性能，$$d_{fake}$$为0，带入损失函数，此时为使损失函数取得最大值，需要辨别器将生成图像辨别为0，将真实图像辨别为1，当回归网络预测出来得分数与真实质量分数得差距小于阈值时，生成器G生成的伪参考图像提升了回归网络的性能，$$d_{fake}$$为1，带入损失函数，此时为使损失函数取得最大值，需要辨别器将生成图像和真实图像均辨别为1。



#### 3. 图像质量预测网络(伪参考图引导的质量回归网络)

==输入是失真图和伪参考图与失真图的差异图==(Dsicrepancy Map)，也即是融入了感知差异信息，可以为回归网络提供丰富的信息，克服了无参考的弊端，极大地指导网络的学习。同时引入==高级语义融合==，将生成器网络G中第N个stack的特征输出(==G的编码块==)同回归网络==最后一个block==的特征输出通过==conca==t进行融合，==防止无意义的伪参考图像影响质量回归，再对融合部分进行最后的全连接，最终输出预测的质量分数==。

损失函数为：

![image-20220228130824648](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220228130824648.png)



### 算法

![IMG_1776(20220228-124505)](C:\Users\Administrator\Desktop\IMG_1776(20220228-124505).JPG)



### 算法评估指标

> SROCC：其中T是失真图像的数量，dt是图像t的GT图的真实质量分数和预测质量分数的等级差。
>
> ![image-20220228131104374](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220228131104374.png)

> LCC：s-t表示GT真实分数均值，另一个表示预测质量分数均值
>
> ![image-20220228131231782](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220228131231782.png)



### 贡献

- ==在质量回归网络中引入了GAN网络，为NR(无参考图)提供了伪参考图；==
- ==在GAN的生成器中加入了质量感知的loss；==
- ==使用判别器引导提高回归网络性能的伪参考图像的生成；==
- ==进行特征融合，防止无意义的伪参考图对其造成影响==



### 数据集

四个IQA常见的数据集上进行了实验LIVE，CSIQ，TID2008，and TID2013

### 对比

相比于ICCV的那篇rankIQA，这篇文章所提出的方法在TID2013上进步巨大。



------



## 四、 RankIQA：Learning from Rankings for No-Reference Image Quality Assessment (ICCV2017)

**对于任何图像对，都知道哪张的质量更高，并由此产生按质量好坏排好序的数据集。**

> 参考链接：**https://www.cnblogs.com/zhangzizi/p/14799877.html**
>
> **https://blog.csdn.net/qq_22565865/article/details/97419888**
>
> **论文链接：https://arxiv.org/abs/1707.08347**



### 摘要

> 本文提出了一种==从排名中学习的无参考图像质量评估方法==，为了解决IQA数据集大小有限的问题，训练了一个==孪生网络==，通过使用==已知相对图像质量排名的数据集==来训练网络并对图像进行排名，这些具有排名的数据集可以不用主观方法去标注，而是自动生成(失真代码)。然后本文把训练好的孪生网络中表示的知识(生成的网络权重，其特征表示的是失真) ==微调（fine-tuning）==到传统的CNN，以此来对单个图像进行图像质量评估的绝对分数的估算。本文还提出了如何通过单个网络向前传播一批图像并反向传播该批次的所有图像对(pairs of images)得出的梯度，从而使本文的方法比传统的孪生网络有效得多。

本文要解决的问题是：对于大样本集，通过人工标注得到图像主观分数需要花费很大的代价，因此训练一个深的CNN网络是非常困难的，解决方法是通过对图像进行不同程度的退化，扩充数据集。

本文==先对好的图像经过不同级别的失真操作，扩充数据集，然后利用该数据集训练孪生网络，将学到的模型权重微调(fine-tuning)到传统CNN中，此时利用标有质量分数的小数据集，提高IQA的准确性。==

同时在训练孪生网络时对反向传播算法进行了改进，==该方法中所有样本只前向传播一次，统计出loss，然后计算梯度进行反向传播。==这种方法被证明能比其他训练方法(hard-negative mining)更好，更快的训练网络。



### 网络结构

![1](C:\Users\Administrator\Desktop\1.jpg)

#### 1.  数据预处理，生成大量ranked images

使用任意的数据集，对图片进行多重失真处理，高斯模糊（GB），高斯噪声（GN），JPEG压缩（JPEG），JPEG2000压缩（JP2K）和快速衰落（FF），==最终目的是生成知道质量排序的数据集。==

在最终的图像对中，虽然并不知道图像的确切质量分数，但是由一张图片产生的一系列失真图片的rank是已知的，谁的质量更高是已知的。



#### 2.  训练孪生网络进行排名

利用高效的孪生网络反向传播，并用一系列排好序的图片训练一个孪生网络，最终得到一个可以==根据图片质量对图片进行排序==的网络。

使用孪生网络来进行图像质量评估排名学习，孪生网络是有两个相同的分支网络，和一个损失函数。两个分支网络==共享权重==（在实际的训练中可以认为是相同的网络，只用实现一个即可）。图像对(Pairs of images)和标签(labels)是输入(也即第一步的图像对和对中的质量排序)，产生两个输出传递给损失函数。损失函数与所有模型参数的梯度通过随机梯度下降反向传播。

具体来说，如果将图像 x 作为网络输入，代表x的输出特征记作 f(x; θ)。这里θ是网络参数，用y来表示图像的真实质量分数，损失函数如下：
$$
L(X_1,X_2;\theta)=max(0,f(X_2;\theta)-f(X_1;\theta)+\varepsilon)
$$
ε是边缘。本文假设$$X_1$$的排名高于$$X_2$$。

给定L关于模型参数θ的梯度，可以使用随机梯度下降（SGD）训练孪生网络。当网络的结果与排名一致时，梯度为零，当网络的结果不一致时，降低质量分数较高的图像的梯度并提高较低分数的梯度。

![image-20220228132453354](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220228132453354.png)

==**高效的反向传播算法：**==

设有n张参考图片，对于传统的孪生网络，所有需要输入网络的数据对：

> (1,2) (1, 3)……(1,n)
>
> (2,1) (2,3)……(2,n)
>
> ………
>
> (n,1) (n,2)……(n,n-1)

共==n(n-1)==张图需要向网络传输

而在本网络中，由于孪生网络的两个分支完全相同，所以每张图片只传输一次，并且只在损失计算层中考虑所有可能的数据对，所有需要输入网络的图像对：即(1,2)与(2,1)是一样的

> (1,2) (1, 3)……(1,n)
>
> (2,3) (2,4)……(2,n)
>
> ………
>
> (n,n-1)



#### 3. 使用IQA数据集训练网络进行微调

==微调==：**提取孪生网络的一个分支，然后利用IQA数据集，也就是有确切质量分数的数据集进行训练。**

训练一个孪生网络对失真图像进行排序后，再从网络中提取单个分支进行微调。给定带有主观标注的小批量M幅图像，将第i幅图像的真值质量分数记为yi，从网络中预测的分数记为y^i。用平方欧氏距离(squared Euclidean distance)作为损失函数对网络进行微调，公式如下：

![image-20220228132936818](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220228132936818.png)

### 数据集

![image-20220228133108603](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220228133108603.png)



### 网络结构

浅层网络有4个卷积层和1个全连接层，对于AlexNet和VGG-16，仅更改输出数量，目标是为每个失真的图像输出一个分数。

![image-20220228133150123](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220228133150123.png)



### 原始数据集

- Waterloo：4,744高质量的自然风景图

  链接：https://ece.uwaterloo.ca/~zduanmu/cvpr16_gmad/

- Places2：365个场景类目，其中在验证集中每类有100张，所有共有36500张，这里只使用验证集。

  链接：http://places2.csail.mit.edu/download.html

- LIVE：包含从29张原图生成的808张图像，这些图像通过5种失真处理：高斯模糊（GB），高斯噪声（GN），JPEG压缩（JPEG），JPEG2000压缩（JP2K）和快速衰落（ FF）。

  链接：http://live.ece.utexas.edu/research/quality/subjective.htm

- TID2013：数据集由25张参考图像和3000种来自24种不同失真类型的失真图像组成。

  链接：http://www.ponomarenko.info/tid2013/tid2013.rar



### Rankiqa-pairwise图像对样本构建

- Live：对waterloo或者Place2的原图用4种失真方法生成pair
- TID2013：对waterloo或者Place2的原图用17种失真方法生成pair，除了＃3，＃4，\＃12，＃13，＃20，＃21，＃24



### Fine-tune微调样本构建

- Live：80%用于训练，20%用于验证，多轮交叉验证
- TID2013：80%用于训练，20%用于验证，多轮交叉验证



### 评估指标

- LCC
- SROCC





------

## 五、 **RAN4IQA: Restorative Adversarial Nets for No-Reference Image Quality Assessment (2018 AAAI)**

**这篇文章解决的是如何将众多的图像块的质量融合为一个分数，结合人眼视觉系统对于图像的敏感性，将图像块的质量进行加权融合。**

> 参考网站：https://blog.csdn.net/qq_34116958/article/details/98969724

> 提出==恢复性对抗网（RAN）==，一个基于GAN的模型，用于无参考图像质量评估（NR-IQA）。RAN模仿了HVS的过程，由三个部分组成：==一个恢复器、一个判别器和一个评估器==。

### 算法流程

类似于那篇CVPR的，采用生成对抗网络来生成恢复图像，将失真图像和恢复图像一起输入到评价网络中，最终生成质量分数。

- ==恢复器对输入的失真图像斑块进行修复和重建，而判别器则将重建的斑块与原始的无失真斑块区分开来。在修复之后，我们观察到，修复后的斑块和失真斑块之间的感知距离是单调的。基于这一现象，我们进一步定义了修复的增益（GoR）。==

- ==评价器通过从失真和修复的斑块中提取特征表示来测量GoR==，从而预测感知分数。

- 最终，==输入图像的质量分数是由各个图像斑块分数的加权总和来估计的==。实验结果在Waterloo Exploration、LIVE和TID2013上的实验结果表明，与最先进的NR-IQA模型相比，RAN的有效性和泛化能力很强。

  

> **==Gain of restoration(GoR)修复增益==**
>
> ==将失真图像和恢复图像之间的差异定为GoR==，实验发现用PSNR和SSIM量化的感知相似度，在图像严重失真时，修复后的图像与失真图像之间的相似度很低，这意味着修复是一个巨大的改进，GoR很高，反之亦然。
>
> 对于特定的恢复网络，失真级别越高，GoR也就越大，目的是对于每个输入，最大化GoR，基于GoR进行感知质量评价，而不是针对恢复图像与原始未失真图像的相似度。



**先从给定的失真图像I中==抽取非重叠斑块==P0、P1…Pn，对于每个块Pk，==恢复对抗网络将其作为输入并试图将Pk还原成相应的无失真原始图像块，而判别器D则将恢复的R(Pk)与原始的是真的图像块区分开来，评估器E将各个图像块的分数做加权和得到最终图像的质量分数。==**



### 网络结构

#### 1. 恢复对抗网络R

==生成网络采用残差块组成==，由于残差连接使恒等函数更容易训练，并且残差网络的结构和HVS恢复的方式十分相似，每个残差块具有相同的布局，两层3*3核。核数为64的卷积，添加分支来积累残差，网络图如下：

输入$$P_k$$是失真的图像块，输出R($$P_k$$)是恢复器生成的结果，

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190809175053192.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MTE2OTU4,size_16,color_FFFFFF,t_70)

恢复对抗网络(RAN)的损失函数包括感知损失和对抗损失：

![image-20220228134200570](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220228134200570.png)

感知损失采用VGG19对图像进行特征提取，计算感知损失：

![image-20220228134216851](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220228134216851.png)

对抗损失采用GAN中的WGAN的损失函数

![image-20220228134305915](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220228134305915.png)



#### 2. 判别器D

采用和VGG13具有相似结构的结构作为对抗网络的组成，判别器的输入是原始的失真图像块和经过生成器处理得到的结果R($$P_k$$)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190809180357808.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MTE2OTU4,size_16,color_FFFFFF,t_70)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190809180439466.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MTE2OTU4,size_16,color_FFFFFF,t_70)

#### 3. 评价网络E

评价网络采用和辨别网络相似的结构.

评价网络的==输入是原始的失真图像块和经过修复器的修复图像块==，从二者中==提取特征表示并将特征向量融合到一起==。在最后阶段，==融合后的特征向量被送入两个分支==，分别计算感知分数(score estimate)和权重(weight estimate)，整个模型收集了所有斑块的加权和。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190809180622211.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MTE2OTU4,size_16,color_FFFFFF,t_70)

网络的输入都是图像的分块，由于图像失真不是均匀的，不同的局部失真不同，因此==最终失真图像的质量分数是对每块的预测分数进行加权和==而不是单纯的平均处理，损失函数如下：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190809180935980.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MTE2OTU4,size_16,color_FFFFFF,t_70)